{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.DriveDataset import DriveDataset\n",
    "from unet import UNet\n",
    "from utils.dice_score import dice_loss\n",
    "from evaluate import evaluate\n",
    "\n",
    "dir_img = Path('./datasets/training/images')\n",
    "# train_label_dir = Path('./datasets/training/1st_manual')\n",
    "dir_mask = Path('./datasets/training/1st_manual')\n",
    "dir_checkpoint = Path('./checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def train(net: UNet,\n",
    "          device,\n",
    "          epochs: int = 5,\n",
    "          batch_size: int = 1,\n",
    "          learning_rate: float = 1e-5,\n",
    "          val_percent: float = 0.1,\n",
    "          save_checkpoint: bool = True,\n",
    "          img_scale: float = 0.5,\n",
    "          amp: bool = False):\n",
    "    dataset = DriveDataset(dir_img, dir_mask, img_scale, mask_suffix='_manual1')\n",
    "\n",
    "    n_val = int(len(dataset)*val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "    experiment = wandb.init(project='DRIVE', resume='allow', anonymous='must')\n",
    "    experiment.config.update(dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                  val_percent=val_percent, save_checkpoint=save_checkpoint, img_scale=img_scale,\n",
    "                                  amp=amp))\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {learning_rate}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_checkpoint}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "        Mixed Precision: {amp}\n",
    "    ''')\n",
    "\n",
    "    # Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=learning_rate, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_step = 0\n",
    "\n",
    "    # Begin training\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                images = batch['image']\n",
    "                masks = batch['mask']\n",
    "                assert images.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                images = images.to(device=device, dtype=torch.float32)\n",
    "                masks = masks.to(device=device, dtype=torch.float32)\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=amp):\n",
    "                    masks_pred = net(images)\n",
    "                    loss = criterion(masks_pred, masks) \\\n",
    "                           + dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
    "                                       F.one_hot(masks, net.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                                       multiclass=True)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "\n",
    "                pbar.update(images.shape[0])\n",
    "                global_step += 1\n",
    "                epoch_loss += loss.item()\n",
    "                experiment.log({\n",
    "                    'train loss': loss.item(),\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                division_step = (n_train // (10 * batch_size))\n",
    "                if division_step > 0:\n",
    "                    if global_step % division_step == 0:\n",
    "                        histograms = {}\n",
    "                        for tag, value in net.named_parameters():\n",
    "                            tag = tag.replace('/', '.')\n",
    "                            histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "                            histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "\n",
    "                        val_score = evaluate(net, val_loader, device)\n",
    "                        scheduler.step(val_score)\n",
    "\n",
    "                        logging.info('Validation Dice score: {}'.format(val_score))\n",
    "                        experiment.log({\n",
    "                            'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                            'validation Dice': val_score,\n",
    "                            'images': wandb.Image(images[0].cpu()),\n",
    "                            'masks': {\n",
    "                                'true': wandb.Image(masks[0].float().cpu()),\n",
    "                                'pred': wandb.Image(torch.softmax(masks_pred, dim=1).argmax(dim=1)[0].float().cpu()),\n",
    "                            },\n",
    "                            'step': global_step,\n",
    "                            'epoch': epoch,\n",
    "                            **histograms\n",
    "                        })\n",
    "        if save_checkpoint:\n",
    "            Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch)))\n",
    "            logging.info(f'Checkpoint {epoch} saved!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cpu\n",
      "INFO: Network:\n",
      "\t3 input channels\n",
      "\t2 output channels (classes)\n",
      "\tTransposed conv upscaling\n",
      "INFO: Creating dataset with 20 images\n",
      "ERROR: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: W&B API key is configured. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Error communicating with wandb process\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m try: wandb.init(settings=wandb.Settings(start_method='fork'))\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m or:  wandb.init(settings=wandb.Settings(start_method='thread'))\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m For more info see: https://docs.wandb.ai/library/init#init-start-error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_12876\\1875837615.py 20 train\n"
     ]
    },
    {
     "ename": "UsageError",
     "evalue": "Error communicating with wandb process\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\nFor more info see: https://docs.wandb.ai/library/init#init-start-error",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUsageError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12876\\1500476529.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m           \u001B[0mimg_scale\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m           \u001B[0mval_percent\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m           amp=False)\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;32mexcept\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'INTERRUPTED.pth'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12876\\1875837615.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(net, device, epochs, batch_size, learning_rate, val_percent, save_checkpoint, img_scale, amp)\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[0mval_loader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDataLoader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdrop_last\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mloader_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m     \u001B[0mexperiment\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mwandb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mproject\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'DRIVE'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresume\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'allow'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0manonymous\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'must'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m     experiment.config.update(dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n\u001B[0;32m     22\u001B[0m                                   \u001B[0mval_percent\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mval_percent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msave_checkpoint\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msave_checkpoint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimg_scale\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mimg_scale\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\software\\anaconda\\anacondasoftware\\envs\\torchCPU\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\u001B[0m in \u001B[0;36minit\u001B[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001B[0m\n\u001B[0;32m    997\u001B[0m         \u001B[0mexcept_exit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mwi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msettings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_except_exit\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    998\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 999\u001B[1;33m             \u001B[0mrun\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mwi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1000\u001B[0m             \u001B[0mexcept_exit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mwi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msettings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_except_exit\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1001\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mKeyboardInterrupt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\software\\anaconda\\anacondasoftware\\envs\\torchCPU\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\u001B[0m in \u001B[0;36minit\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    651\u001B[0m                     \u001B[0mbackend\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcleanup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    652\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mteardown\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 653\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mUsageError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror_message\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    654\u001B[0m             \u001B[1;32massert\u001B[0m \u001B[0mrun_result\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mrun_result\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    655\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mrun_result\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresumed\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUsageError\u001B[0m: Error communicating with wandb process\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\nFor more info see: https://docs.wandb.ai/library/init#init-start-error"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f'Using device {device}')\n",
    "\n",
    "net = UNet(n_channels=3, n_classes=2, bilinear=False)\n",
    "\n",
    "logging.info(f'Network:\\n'\n",
    "             f'\\t{net.n_channels} input channels\\n'\n",
    "             f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "             f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n",
    "\n",
    "net.to(device=device)\n",
    "\n",
    "try:\n",
    "    train(net=net,\n",
    "          epochs=2,\n",
    "          batch_size=1,\n",
    "          learning_rate=1e-5,\n",
    "          device=device,\n",
    "          img_scale=1,\n",
    "          val_percent=0.1,\n",
    "          amp=False)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "    logging.info('Saved interrupt')\n",
    "    raise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}